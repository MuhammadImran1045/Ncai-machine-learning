{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "Smart Translator.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadImran1045/Ncai-machine-learning/blob/master/Smart_Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz0D4BgzKirC",
        "colab_type": "text"
      },
      "source": [
        "#**Smart Languahe Translation**\n",
        "#**translation one into 12 different languages**#\n",
        "**Using Neural Machine Tranlation**\n",
        "\n",
        "**Recurrent Neural Network (RNN)**\n",
        "\n",
        "**Sequence to sequence Model**\n",
        "\n",
        "**Encoder and Decoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx0W8xFkT6bL",
        "colab_type": "text"
      },
      "source": [
        "##**LOADING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6nNjkl8TWzc",
        "colab_type": "code",
        "outputId": "2949c81b-5fdf-4d58-da63-4b57651fe9f9",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3a329a8d-4010-40c7-b546-9ef63f54cb26\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3a329a8d-4010-40c7-b546-9ef63f54cb26\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving deu.txt to deu.txt\n",
            "Saving fin.txt to fin.txt\n",
            "Saving fra.txt to fra.txt\n",
            "Saving hun.txt to hun.txt\n",
            "Saving ita.txt to ita.txt\n",
            "Saving nld.txt to nld.txt\n",
            "Saving por.txt to por.txt\n",
            "Saving rus.txt to rus.txt\n",
            "Saving spa.txt to spa.txt\n",
            "Saving swe.txt to swe.txt\n",
            "Saving tur.txt to tur.txt\n",
            "Saving ukr.txt to ukr.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqxLcV0mUolP",
        "colab_type": "code",
        "outputId": "6567754d-d14c-458b-8e95-148333696154",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 57
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-05aebbe9-82d4-4b8b-a168-e7fdb15dc0d6\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-05aebbe9-82d4-4b8b-a168-e7fdb15dc0d6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving deu.txt to deu.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGUcwMwFODcn",
        "colab_type": "text"
      },
      "source": [
        "# **English to French Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oRzlfvfJoGO",
        "colab_type": "code",
        "outputId": "cd2e14c3-b0c8-4517-ac11-ec0f2ecb205c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('fra.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and french sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each french character - key is index and value is french character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.9328 - val_loss: 0.9679\n",
            "Epoch 2/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.7343 - val_loss: 0.7833\n",
            "Epoch 3/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.6239 - val_loss: 0.7157\n",
            "Epoch 4/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5670 - val_loss: 0.6646\n",
            "Epoch 5/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5250 - val_loss: 0.6346\n",
            "Epoch 6/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.4925 - val_loss: 0.6060\n",
            "Epoch 7/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4661 - val_loss: 0.5793\n",
            "Epoch 8/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.4434 - val_loss: 0.5657\n",
            "Epoch 9/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4238 - val_loss: 0.5479\n",
            "Epoch 10/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.4057 - val_loss: 0.5381\n",
            "Epoch 11/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3897 - val_loss: 0.5254\n",
            "Epoch 12/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3750 - val_loss: 0.5147\n",
            "Epoch 13/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3611 - val_loss: 0.5053\n",
            "Epoch 14/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3485 - val_loss: 0.5071\n",
            "Epoch 15/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3362 - val_loss: 0.4979\n",
            "Epoch 16/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3250 - val_loss: 0.4905\n",
            "Epoch 17/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3140 - val_loss: 0.4867\n",
            "Epoch 18/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3040 - val_loss: 0.4843\n",
            "Epoch 19/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2941 - val_loss: 0.4850\n",
            "Epoch 20/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2846 - val_loss: 0.4866\n",
            "Epoch 21/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2765 - val_loss: 0.4906\n",
            "Epoch 22/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2678 - val_loss: 0.4808\n",
            "Epoch 23/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2597 - val_loss: 0.4772\n",
            "Epoch 24/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2521 - val_loss: 0.4786\n",
            "Epoch 25/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2448 - val_loss: 0.4817\n",
            "Epoch 26/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2375 - val_loss: 0.4858\n",
            "Epoch 27/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2311 - val_loss: 0.4845\n",
            "Epoch 28/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2240 - val_loss: 0.4979\n",
            "Epoch 29/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2183 - val_loss: 0.4936\n",
            "Epoch 30/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2119 - val_loss: 0.4941\n",
            "Epoch 31/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2062 - val_loss: 0.5025\n",
            "Epoch 32/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.2004 - val_loss: 0.5051\n",
            "Epoch 33/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1953 - val_loss: 0.5036\n",
            "Epoch 34/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1899 - val_loss: 0.5094\n",
            "Epoch 35/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1845 - val_loss: 0.5081\n",
            "Epoch 36/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1802 - val_loss: 0.5138\n",
            "Epoch 37/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1756 - val_loss: 0.5289\n",
            "Epoch 38/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1709 - val_loss: 0.5240\n",
            "Epoch 39/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1667 - val_loss: 0.5275\n",
            "Epoch 40/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1628 - val_loss: 0.5339\n",
            "Epoch 41/50\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1588 - val_loss: 0.5348\n",
            "Epoch 42/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1549 - val_loss: 0.5443\n",
            "Epoch 43/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1512 - val_loss: 0.5481\n",
            "Epoch 44/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1479 - val_loss: 0.5449\n",
            "Epoch 45/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1441 - val_loss: 0.5514\n",
            "Epoch 46/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1409 - val_loss: 0.5539\n",
            "Epoch 47/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1378 - val_loss: 0.5609\n",
            "Epoch 48/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1350 - val_loss: 0.5717\n",
            "Epoch 49/50\n",
            "8000/8000 [==============================] - 12s 1ms/step - loss: 0.1319 - val_loss: 0.5740\n",
            "Epoch 50/50\n",
            "8000/8000 [==============================] - 12s 2ms/step - loss: 0.1294 - val_loss: 0.5757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7siInVIJoHr",
        "colab_type": "code",
        "outputId": "4bbdb15e-381e-4efc-981b-f599649e0e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: ﻿Go.\n",
            "Decoded sentence: Ares pais !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Ares pais !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Ares pais !\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Ares pais !\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Ares -ous !\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Aous sous !\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: es pais !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Ales -ous !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Ales -ous !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Ales -ous !\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Ares pais !\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Ares pais !\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yT9zblFJoHx",
        "colab_type": "code",
        "outputId": "d88f8bcd-c75a-4bae-e923-447ad002cdb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fra_sent"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\tSpring!\\n',\n",
              " '\\tVem?\\n',\n",
              " '\\tHjälp!\\n',\n",
              " '\\tHoppa!\\n',\n",
              " '\\tStanna!\\n',\n",
              " '\\tVänta!\\n',\n",
              " '\\tVänta.\\n',\n",
              " '\\tJag förstår.\\n',\n",
              " '\\tJag ser.\\n',\n",
              " '\\tJag vann!\\n',\n",
              " '\\tLe.\\n',\n",
              " '\\tSkål!\\n',\n",
              " '\\tHan sprang.\\n',\n",
              " '\\tAldrig i livet!\\n',\n",
              " '\\tÄr det sant?\\n',\n",
              " '\\tJaså?\\n',\n",
              " '\\tTack.\\n',\n",
              " '\\tVi försöker.\\n',\n",
              " '\\tFråga Tom.\\n',\n",
              " '\\tTa det lugnt.\\n',\n",
              " '\\tVar trevlig.\\n',\n",
              " '\\tKom igen.\\n',\n",
              " '\\tSläpp det!\\n',\n",
              " '\\tHämta Tom.\\n',\n",
              " '\\tUt med dig!\\n',\n",
              " '\\tUt med er!\\n',\n",
              " '\\tGå iväg.\\n',\n",
              " '\\tHan springer.\\n',\n",
              " '\\tJag håller med.\\n',\n",
              " '\\tJag instämmer.\\n',\n",
              " '\\tDet är okej.\\n',\n",
              " '\\tKyss mig.\\n',\n",
              " '\\tJag också.\\n',\n",
              " '\\tPerfekt!\\n',\n",
              " '\\tVisa mig.\\n',\n",
              " '\\tBerätta för mig.\\n',\n",
              " '\\tVakna!\\n',\n",
              " '\\tTvätta dig.\\n',\n",
              " '\\tVi förlorade.\\n',\n",
              " '\\tSluta!\\n',\n",
              " '\\tLägg av!\\n',\n",
              " '\\tVar still.\\n',\n",
              " '\\tStilla.\\n',\n",
              " '\\tRyck upp dig!\\n',\n",
              " '\\tRyck upp er!\\n',\n",
              " '\\tKör vidare.\\n',\n",
              " '\\tHitta Tom.\\n',\n",
              " '\\tFixa det här.\\n',\n",
              " '\\tLaga den här.\\n',\n",
              " '\\tDucka.\\n',\n",
              " '\\tStick!\\n',\n",
              " '\\tFortsätt in.\\n',\n",
              " '\\tHugg tag i Tom.\\n',\n",
              " '\\tHa det så roligt.\\n',\n",
              " '\\tBlidka mig.\\n',\n",
              " '\\tGör mig till viljes.\\n',\n",
              " '\\tSkynda dig!\\n',\n",
              " '\\tJag glömde.\\n',\n",
              " '\\tJag förstår.\\n',\n",
              " '\\tJag stannade kvar.\\n',\n",
              " '\\tJag blev kvar.\\n',\n",
              " '\\tJag använder det.\\n',\n",
              " '\\tJag betalar.\\n',\n",
              " '\\tJag är upptagen.\\n',\n",
              " '\\tJag fryser.\\n',\n",
              " '\\tDet är okej med mig.\\n',\n",
              " '\\tJag är ledig.\\n',\n",
              " '\\tJag är mätt.\\n',\n",
              " '\\tJag är hemma.\\n',\n",
              " '\\tJag är sen.\\n',\n",
              " '\\tJag är nästa.\\n',\n",
              " '\\tJag är närmast i tur.\\n',\n",
              " '\\tJag är varm.\\n',\n",
              " '\\tDet hjälper.\\n',\n",
              " '\\tDet är Tom.\\n',\n",
              " '\\tDet är roligt.\\n',\n",
              " '\\tDet är kul.\\n',\n",
              " '\\tDet är skoj.\\n',\n",
              " '\\tDet är skojigt.\\n',\n",
              " '\\tKyss Tom.\\n',\n",
              " '\\tLåt det vara.\\n',\n",
              " '\\tLåt den vara.\\n',\n",
              " '\\tLämna mig.\\n',\n",
              " '\\tLämna oss.\\n',\n",
              " '\\tGift dig med mig.\\n',\n",
              " '\\tKan jag gå?\\n',\n",
              " '\\tVar beredd.\\n',\n",
              " '\\tHåll dig redo.\\n',\n",
              " '\\tHåll dig i närheten.\\n',\n",
              " '\\tStå upp.\\n',\n",
              " '\\tStäll dig upp!\\n',\n",
              " '\\tStäll er upp!\\n',\n",
              " '\\tStanna kvar här.\\n',\n",
              " '\\tStanna kvar.\\n',\n",
              " '\\tSitt kvar.\\n',\n",
              " '\\tStå kvar.\\n',\n",
              " '\\tLigg kvar.\\n',\n",
              " '\\tTom ljuger.\\n',\n",
              " '\\tTom grät.\\n',\n",
              " '\\tTom är uppe.\\n',\n",
              " '\\tAnvänd den här.\\n',\n",
              " '\\tAnvänd det här.\\n',\n",
              " '\\tAnvänd denna.\\n',\n",
              " '\\tAnvänd detta.\\n',\n",
              " '\\tAnvänd den här!\\n',\n",
              " '\\tVarna Tom.\\n',\n",
              " '\\tSe på mig.\\n',\n",
              " '\\tTitta på oss.\\n',\n",
              " '\\tSe på oss.\\n',\n",
              " '\\tVem dog?\\n',\n",
              " '\\tVem har dött?\\n',\n",
              " '\\tVem slutade?\\n',\n",
              " '\\tVem är han?\\n',\n",
              " '\\tSkriv till mig.\\n',\n",
              " '\\tSikta. Skjut!\\n',\n",
              " '\\tSvara.\\n',\n",
              " '\\tSätt er.\\n',\n",
              " '\\tSätt dig.\\n',\n",
              " '\\tFåglar flyger.\\n',\n",
              " '\\tGud välsigne dig.\\n',\n",
              " '\\tLugna ner dig.\\n',\n",
              " '\\tLugna ner er.\\n',\n",
              " '\\tKom tillbaka!\\n',\n",
              " '\\tLugna dig.\\n',\n",
              " '\\tGör det nu.\\n',\n",
              " '\\tGråt inte.\\n',\n",
              " '\\tDö inte.\\n',\n",
              " '\\tLjug inte.\\n',\n",
              " '\\tSpring inte!\\n',\n",
              " '\\tUrsäkta mig.\\n',\n",
              " '\\tGlöm det.\\n',\n",
              " '\\tTa tag i den där.\\n',\n",
              " '\\tTa lite.\\n',\n",
              " '\\tHan är dj.\\n',\n",
              " '\\tJag är kall.\\n',\n",
              " '\\tJag kan springa.\\n',\n",
              " '\\tJag kan åka skidor.\\n',\n",
              " '\\tJag svimmade.\\n',\n",
              " '\\tJag ger upp.\\n',\n",
              " '\\tJag träffades.\\n',\n",
              " '\\tJag blev träffad.\\n',\n",
              " '\\tJag älskar det.\\n',\n",
              " '\\tJag lovar.\\n',\n",
              " '\\tJag såg en.\\n',\n",
              " '\\tJag såg ett.\\n',\n",
              " '\\tJag ska ringa.\\n',\n",
              " '\\tJag ringer.\\n',\n",
              " '\\tJag lagar mat.\\n',\n",
              " '\\tJag fixar maten.\\n',\n",
              " '\\tJag överlever.\\n',\n",
              " '\\tJag överlever nog.\\n',\n",
              " '\\tJag är arg.\\n',\n",
              " '\\tJag är vaken.\\n',\n",
              " '\\tJag har tråkigt.\\n',\n",
              " '\\tJag är pank.\\n',\n",
              " '\\tJag är lycklig.\\n',\n",
              " '\\tJag är tyst.\\n',\n",
              " '\\tJag är tystlåten.\\n',\n",
              " '\\tJag är redo!\\n',\n",
              " '\\tJag är färdig!\\n',\n",
              " '\\tJag är klar!\\n',\n",
              " '\\tJag har rätt.\\n',\n",
              " '\\tJag är nykter.\\n',\n",
              " '\\tFörlåt.\\n',\n",
              " '\\tJag är trött.\\n',\n",
              " '\\tJag är upprörd.\\n',\n",
              " '\\tJag är ung.\\n',\n",
              " '\\tÄr det illa?\\n',\n",
              " '\\tDet snöade.\\n',\n",
              " '\\tKlockan är 9.15.\\n',\n",
              " '\\tDen är sval.\\n',\n",
              " '\\tDen är cool.\\n',\n",
              " '\\tDet är mörkt.\\n',\n",
              " '\\tDet är rättvist.\\n',\n",
              " '\\tDet är okej.\\n',\n",
              " '\\tDet är gratis.\\n',\n",
              " '\\tDet är bra.\\n',\n",
              " '\\tDen är bra.\\n',\n",
              " '\\tDet är gott.\\n',\n",
              " '\\tDen är god.\\n',\n",
              " '\\tDen är här.\\n',\n",
              " '\\tDen har kommit.\\n',\n",
              " '\\tHär är den.\\n',\n",
              " '\\tDen är enorm.\\n',\n",
              " '\\tDen är jättestor.\\n',\n",
              " '\\tDet är jättestort.\\n',\n",
              " '\\tDet är enormt.\\n',\n",
              " '\\tDet är mitt.\\n',\n",
              " '\\tDen är min.\\n',\n",
              " '\\tDet är okej.\\n',\n",
              " '\\tDen är vår.\\n',\n",
              " '\\tDet är vårt.\\n',\n",
              " '\\tDen är äkta.\\n',\n",
              " '\\tDet är äkta.\\n',\n",
              " '\\tDen är verklig.\\n',\n",
              " '\\tDet är sand.\\n',\n",
              " '\\tDet är dags.\\n',\n",
              " '\\tTiden är inne.\\n',\n",
              " '\\tDet är sant.\\n',\n",
              " '\\tDet är sant.\\n',\n",
              " '\\tHåll dig nere.\\n',\n",
              " '\\tHåll er nere.\\n',\n",
              " '\\tBehåll den här.\\n',\n",
              " '\\tBehåll det här.\\n',\n",
              " '\\tHåll dig varm.\\n',\n",
              " '\\tHåll er varma.\\n',\n",
              " '\\tSläpp det.\\n',\n",
              " '\\tLåt mig gå!\\n',\n",
              " '\\tSläpp in mig.\\n',\n",
              " '\\tVi försöker.\\n',\n",
              " '\\tTitta bort.\\n',\n",
              " '\\tFlytta på dig.\\n',\n",
              " '\\tSjälvklart!\\n',\n",
              " '\\tSjälvfallet!\\n',\n",
              " '\\tSjälvklart.\\n',\n",
              " '\\tAllvarligt?\\n',\n",
              " '\\tSeriöst?\\n',\n",
              " '\\tHen grät.\\n',\n",
              " '\\tSitt stilla.\\n',\n",
              " '\\tSitt still.\\n',\n",
              " '\\tHåll dig borta.\\n',\n",
              " '\\tHåll er borta.\\n',\n",
              " '\\tStanna här.\\n',\n",
              " '\\tSluta med det där.\\n',\n",
              " '\\tSluta upp med det där.\\n',\n",
              " '\\tTa min.\\n',\n",
              " '\\tTa mitt.\\n',\n",
              " '\\tTag min.\\n',\n",
              " '\\tTag mitt.\\n',\n",
              " '\\tTack.\\n',\n",
              " '\\tDet är okej.\\n',\n",
              " '\\tDet är lugnt.\\n',\n",
              " '\\tSen då?\\n',\n",
              " '\\tSedan då?\\n',\n",
              " '\\tDe simmade.\\n',\n",
              " '\\tTom grät.\\n',\n",
              " '\\tTom simmar.\\n',\n",
              " '\\tTom är tjock.\\n',\n",
              " '\\tTom är fet.\\n',\n",
              " '\\tTom är arg.\\n',\n",
              " '\\tTom är ledsen.\\n',\n",
              " '\\tTom är blyg.\\n',\n",
              " '\\tFörsök igen.\\n',\n",
              " '\\tSväng vänster.\\n',\n",
              " '\\tVi misslyckades.\\n',\n",
              " '\\tVi glömde.\\n',\n",
              " '\\tVi väntade.\\n',\n",
              " '\\tVi kommer att vinna.\\n',\n",
              " '\\tVi har vunnit!\\n',\n",
              " '\\tHur är läget?\\n',\n",
              " '\\tVad händer?\\n',\n",
              " '\\tVem är han?\\n',\n",
              " '\\tVem vet?\\n',\n",
              " '\\tVem är hon?\\n',\n",
              " '\\tDin idiot!\\n',\n",
              " '\\tHar jag fel?\\n',\n",
              " '\\tMår du bra?\\n',\n",
              " '\\tVar belåten.\\n',\n",
              " '\\tKom i tid.\\n',\n",
              " '\\tVar tålmodig.\\n',\n",
              " '\\tHa tålamod.\\n',\n",
              " '\\tVar tålmodiga.\\n',\n",
              " '\\tVar allvarlig.\\n',\n",
              " '\\tFåglar sjunger.\\n',\n",
              " '\\tHämta mat.\\n',\n",
              " '\\tHämta hjälp.\\n',\n",
              " '\\tGå och hämta hjälp.\\n',\n",
              " '\\tGå och hämta vin.\\n',\n",
              " '\\tHämta vin.\\n',\n",
              " '\\tFår jag hjälpa?\\n',\n",
              " '\\tBär den här.\\n',\n",
              " '\\tKolla det där.\\n',\n",
              " '\\tKolla det här.\\n',\n",
              " '\\tKom ensam.\\n',\n",
              " '\\tSjälvfallet!\\n',\n",
              " '\\tAbsolut!\\n',\n",
              " '\\tHoppa inte!\\n',\n",
              " '\\tTitta inte.\\n',\n",
              " '\\tRör er inte!\\n',\n",
              " '\\tRör dig inte!\\n',\n",
              " '\\tKnuffas inte.\\n',\n",
              " '\\tSjung inte.\\n',\n",
              " '\\tStanna inte.\\n',\n",
              " '\\tPrata inte!\\n',\n",
              " '\\tTala inte.\\n',\n",
              " '\\tVänta inte.\\n',\n",
              " '\\tSkrik inte.\\n',\n",
              " '\\tPlikten kallar.\\n',\n",
              " '\\tÄt långsamt.\\n',\n",
              " '\\tÄt sakta.\\n',\n",
              " '\\tUndersök det.\\n',\n",
              " '\\tGå igenom den.\\n',\n",
              " '\\tGranska den.\\n',\n",
              " '\\tFölj Tom.\\n',\n",
              " '\\tFölj efter Tom.\\n',\n",
              " '\\tGlöm Tom.\\n',\n",
              " '\\tGlöm honom.\\n',\n",
              " '\\tFörlåt oss.\\n',\n",
              " '\\tSkaffa dig ett liv.\\n',\n",
              " '\\tLåt mig vara.\\n',\n",
              " '\\tRör mig inte.\\n',\n",
              " '\\tLåt bli mig.\\n',\n",
              " '\\tGå och tvätta dig!\\n',\n",
              " '\\tGud finns.\\n',\n",
              " '\\tGud existerar.\\n',\n",
              " '\\tHan är snäll.\\n',\n",
              " '\\tHan är vänlig.\\n',\n",
              " '\\tHan klarade det.\\n',\n",
              " '\\tHan hann.\\n',\n",
              " '\\tHan är schweizare.\\n',\n",
              " '\\tHan ljuger.\\n',\n",
              " '\\tHan är smart.\\n',\n",
              " '\\tHan är intelligent.\\n',\n",
              " '\\tVad smart!\\n',\n",
              " '\\tSå tragiskt!\\n',\n",
              " '\\tSkynda dig tillbaka.\\n',\n",
              " '\\tJag har rätt.\\n',\n",
              " '\\tJag kan stanna.\\n',\n",
              " '\\tJag får stanna.\\n',\n",
              " '\\tJag kan simma.\\n',\n",
              " '\\tJag kan vänta.\\n',\n",
              " '\\tDet tvivlar jag på.\\n',\n",
              " '\\tJag tvivlar på det.\\n',\n",
              " '\\tJag äter här.\\n',\n",
              " '\\tJag känner mig vältränad.\\n',\n",
              " '\\tJag känner mig ledsen.\\n',\n",
              " '\\tJag hatar dig.\\n',\n",
              " '\\tJag gillar honom.\\n',\n",
              " '\\tJag gillar te.\\n',\n",
              " '\\tJag tycker om te.\\n',\n",
              " '\\tJag älskar henne.\\n',\n",
              " '\\tJag älskar honom.\\n',\n",
              " '\\tJag älskar dig.\\n',\n",
              " '\\tJag älskar dej.\\n',\n",
              " '\\tJag saknar dig.\\n',\n",
              " '\\tJag behöver is.\\n',\n",
              " '\\tJag behöver dig.\\n',\n",
              " '\\tJag behöver er.\\n',\n",
              " '\\tJag fick panik.\\n',\n",
              " '\\tJag lovade.\\n',\n",
              " '\\tJag minns.\\n',\n",
              " '\\tJag kommer ihåg.\\n',\n",
              " '\\tJag överlevde.\\n',\n",
              " '\\tJag tror det.\\n',\n",
              " '\\tJag ska kolla.\\n',\n",
              " '\\tJag fixar det.\\n',\n",
              " '\\tJag ska göra det.\\n',\n",
              " '\\tJag gör det.\\n',\n",
              " '\\tJag kommer att göra det.\\n',\n",
              " '\\tJag kommer.\\n',\n",
              " '\\tJag är hungrig.\\n',\n",
              " '\\tJag är utvilad.\\n',\n",
              " '\\tÄr Tom arg?\\n',\n",
              " '\\tÄr Tom galen?\\n',\n",
              " '\\tÄr den blå?\\n',\n",
              " '\\tÄr det blått?\\n',\n",
              " '\\tÄr det gratis?\\n',\n",
              " '\\tÄr det kärlek?\\n',\n",
              " '\\tÄr det dags?\\n',\n",
              " '\\tÄr tiden inne?\\n',\n",
              " '\\tÄr det sant?\\n',\n",
              " '\\tJaså!\\n',\n",
              " '\\tDet händer.\\n',\n",
              " '\\tDet är mitt.\\n',\n",
              " '\\tDet är tomt.\\n',\n",
              " '\\tDet är blankt.\\n',\n",
              " '\\tDet står stilla.\\n',\n",
              " '\\tDet är uppfattat.\\n',\n",
              " '\\tDet är tydligt.\\n',\n",
              " '\\tDet är uppenbart.\\n',\n",
              " '\\tDen är tom.\\n',\n",
              " '\\tDet är tomt.\\n',\n",
              " '\\tDet är min cd-skiva.\\n',\n",
              " '\\tDet är min cd.\\n',\n",
              " '\\tJag bjuder.\\n',\n",
              " '\\tDet är säkrare.\\n',\n",
              " '\\tDet är läskigt.\\n',\n",
              " '\\tDen är läskig.\\n',\n",
              " '\\tDen glänser.\\n',\n",
              " '\\tDen sitter fast.\\n',\n",
              " '\\tDet sitter fast.\\n',\n",
              " '\\tDär är den.\\n',\n",
              " '\\tDet är där.\\n',\n",
              " '\\tDen är där.\\n',\n",
              " '\\tGör det bara.\\n',\n",
              " '\\tTa anteckningar.\\n',\n",
              " '\\tHåll still.\\n',\n",
              " '\\tSläpp honom!\\n',\n",
              " '\\tLåt den torka.\\n',\n",
              " '\\tLåt det torka.\\n',\n",
              " '\\tSläpp ut mig!\\n',\n",
              " '\\tLåt mig betala.\\n',\n",
              " '\\tLåt mig se.\\n',\n",
              " '\\tRyck upp dig.\\n',\n",
              " '\\tTitta på mig.\\n',\n",
              " '\\tKärlek gör ont.\\n',\n",
              " '\\tInga problem!\\n',\n",
              " '\\tIn med dig nu.\\n',\n",
              " '\\tKom in nu.\\n',\n",
              " '\\tSkynda dig!\\n',\n",
              " '\\tSpring för livet!\\n',\n",
              " '\\tSäg: ”kan jag få”.\\n',\n",
              " '\\tHon log.\\n',\n",
              " '\\tHon är upptagen.\\n',\n",
              " '\\tSjung med.\\n',\n",
              " '\\tStanna där.\\n',\n",
              " '\\tTa en buss.\\n',\n",
              " '\\tTa skydd!\\n',\n",
              " '\\tPrata med mig!\\n',\n",
              " '\\tDet är sorgligt.\\n',\n",
              " '\\tDet är trist.\\n',\n",
              " '\\tDe stod.\\n',\n",
              " '\\tDe röstade.\\n',\n",
              " '\\tTiden flyger.\\n',\n",
              " '\\tTiden rusar.\\n',\n",
              " '\\tTiden är ute.\\n',\n",
              " '\\tTom fuskar.\\n',\n",
              " '\\tTom kör.\\n',\n",
              " '\\tTom steg upp.\\n',\n",
              " '\\tTom slog mig.\\n',\n",
              " '\\tTom är ute.\\n',\n",
              " '\\tTom nickade.\\n',\n",
              " '\\tTom ringde.\\n',\n",
              " '\\tTom blinkade.\\n',\n",
              " '\\tTom betalar nog.\\n',\n",
              " '\\tTom är död.\\n',\n",
              " '\\tTom är döv.\\n',\n",
              " '\\tTom har dött.\\n',\n",
              " '\\tTom är snabb.\\n',\n",
              " '\\tTom är fri.\\n',\n",
              " '\\tTom är glad.\\n',\n",
              " '\\tTom är borta.\\n',\n",
              " '\\tTom har åkt.\\n',\n",
              " '\\tTom har gått.\\n',\n",
              " '\\tTom är här.\\n',\n",
              " '\\tTom är hemma.\\n',\n",
              " '\\tTom är skadad.\\n',\n",
              " '\\tTom har åkt.\\n',\n",
              " '\\tTom är elak.\\n',\n",
              " '\\tTom är säker.\\n',\n",
              " '\\tTom är sjuk.\\n',\n",
              " '\\tTom är svag.\\n',\n",
              " '\\tSväng höger.\\n',\n",
              " '\\tVänta ett tag.\\n',\n",
              " '\\tTitta på det här.\\n',\n",
              " '\\tKolla på det här.\\n',\n",
              " '\\tVi är män.\\n',\n",
              " '\\tVi är här.\\n',\n",
              " '\\tVad är nytt?\\n',\n",
              " '\\tVar är jag?\\n',\n",
              " '\\tVem är hon?\\n',\n",
              " '\\tDu lyckades!\\n',\n",
              " '\\tDu kan gå.\\n',\n",
              " '\\tÄr du galen?\\n',\n",
              " '\\tÄr du arg?\\n',\n",
              " '\\tÄr ni arga?\\n',\n",
              " '\\tÄr du ny?\\n',\n",
              " '\\tÄr du ledsen?\\n',\n",
              " '\\tVar kreativ.\\n',\n",
              " '\\tVar kreativa.\\n',\n",
              " '\\tVar diskret.\\n',\n",
              " '\\tVar diskreta.\\n',\n",
              " '\\tVar vänlig.\\n',\n",
              " '\\tVar vänliga.\\n',\n",
              " '\\tVar generös.\\n',\n",
              " '\\tVar generösa.\\n',\n",
              " '\\tVar nådig.\\n',\n",
              " '\\tVar redo.\\n',\n",
              " '\\tVar punktlig.\\n',\n",
              " '\\tVar punktliga.\\n',\n",
              " '\\tVar hänsynslös.\\n',\n",
              " '\\tVar hänsynslösa.\\n',\n",
              " '\\tVar förnuftig.\\n',\n",
              " '\\tVar specifik.\\n',\n",
              " '\\tVar tydlig.\\n',\n",
              " '\\tVar tolerant.\\n',\n",
              " '\\tVar toleranta.\\n',\n",
              " '\\tVar vaksam.\\n',\n",
              " '\\tVar vaksamma.\\n',\n",
              " '\\tVar på din vakt.\\n',\n",
              " '\\tVar på er vakt.\\n',\n",
              " '\\tÖl är gott.\\n',\n",
              " '\\tKolla en gång till.\\n',\n",
              " '\\tKom ombord.\\n',\n",
              " '\\tKom närmare.\\n',\n",
              " '\\tKom in.\\n',\n",
              " '\\tTrösta Tom.\\n',\n",
              " '\\tKontakta Tom.\\n',\n",
              " '\\tTa kontakt med Tom.\\n',\n",
              " '\\tVann du?\\n',\n",
              " '\\tVann ni?\\n',\n",
              " '\\tGör det igen!\\n',\n",
              " '\\tGör det igen.\\n',\n",
              " '\\tGör det senare.\\n',\n",
              " '\\tGör det rätt.\\n',\n",
              " '\\tInga men.\\n',\n",
              " '\\tFuska inte.\\n',\n",
              " '\\tGör det inte!\\n',\n",
              " '\\tGör det inte.\\n',\n",
              " '\\tSlåss inte.\\n',\n",
              " '\\tGlo inte.\\n',\n",
              " '\\tGå inte.\\n',\n",
              " '\\tSkjut inte!\\n',\n",
              " '\\tLe inte.\\n',\n",
              " '\\tTala inte.\\n',\n",
              " '\\tPrata inte.\\n',\n",
              " '\\tStå inte.\\n',\n",
              " '\\tStirra inte.\\n',\n",
              " '\\tOroa dig inte.\\n',\n",
              " '\\tByt om.\\n',\n",
              " '\\tByt kläder.\\n',\n",
              " '\\tKlä på dig.\\n',\n",
              " '\\tKlä på er.\\n',\n",
              " '\\tGå och hämta hjälp.\\n',\n",
              " '\\tGå med Tom.\\n',\n",
              " '\\tFölj med Tom.\\n',\n",
              " '\\tHan får komma.\\n',\n",
              " '\\tHan kan simma.\\n',\n",
              " '\\tHan ljuger.\\n',\n",
              " '\\tHan gillar mig.\\n',\n",
              " '\\tHan älskar mig.\\n',\n",
              " '\\tHan blev skadad.\\n',\n",
              " '\\tHan blev sårad.\\n',\n",
              " '\\tHan låtsas.\\n',\n",
              " '\\tHan är stark.\\n',\n",
              " '\\tHär är hon!\\n',\n",
              " '\\tHur är det?\\n',\n",
              " '\\tHur mår du?\\n',\n",
              " '\\tJag gick också.\\n',\n",
              " '\\tJag kommer.\\n',\n",
              " '\\tJag är längre.\\n',\n",
              " '\\tJag kan köra.\\n',\n",
              " '\\tJag kan inte äta.\\n',\n",
              " '\\tJag äter frukt.\\n',\n",
              " '\\tJag fick ett jobb.\\n',\n",
              " '\\tJag skaffade mig ett jobb.\\n',\n",
              " '\\tJag fastnade.\\n',\n",
              " '\\tJag blev trött.\\n',\n",
              " '\\tJag hade en katt.\\n',\n",
              " '\\tJag hatar arbete.\\n',\n",
              " '\\tJag gillar båda.\\n',\n",
              " '\\tJag gillar jazz.\\n',\n",
              " '\\tJag gillar det här.\\n',\n",
              " '\\tJag bor här.\\n',\n",
              " '\\tJag älskar rock.\\n',\n",
              " '\\tJag gråter aldrig.\\n',\n",
              " '\\tJag försov mig.\\n',\n",
              " '\\tJag hämtade mig.\\n',\n",
              " '\\tJag ordnade det.\\n',\n",
              " '\\tJag säljer bilar.\\n',\n",
              " '\\tJag tackar dig.\\n',\n",
              " '\\tJag tackar er.\\n',\n",
              " '\\tJag litar på dig.\\n',\n",
              " '\\tJag litar på er.\\n',\n",
              " '\\tJag var arg.\\n',\n",
              " '\\tJag blev avskedad.\\n',\n",
              " '\\tJag hade tur.\\n',\n",
              " '\\tJag var trött.\\n',\n",
              " '\\tJag arbetar här.\\n',\n",
              " '\\tJag jobbar här.\\n',\n",
              " '\\tJag ska byta om.\\n',\n",
              " '\\tJag klarar mig.\\n',\n",
              " '\\tJag hämtar den.\\n',\n",
              " '\\tJag hämtar det.\\n',\n",
              " '\\tJag är vegan.\\n',\n",
              " '\\tJag är hemma.\\n',\n",
              " '\\tJag är säker.\\n',\n",
              " '\\tJag är nyfiken.\\n',\n",
              " '\\tJag är ursinnig.\\n',\n",
              " '\\tJag är rasande.\\n',\n",
              " '\\tJag är hälsosam.\\n',\n",
              " '\\tJag läser.\\n',\n",
              " '\\tJag är seriös.\\n',\n",
              " '\\tJag är allvarlig.\\n',\n",
              " '\\tJag är så mätt.\\n',\n",
              " '\\tJag är utsvulten.\\n',\n",
              " '\\tJag är törstig.\\n',\n",
              " '\\tJag är rörd.\\n',\n",
              " '\\tJag vinner.\\n',\n",
              " '\\tÄr Tom näst i tur?\\n',\n",
              " '\\tÄr den vit?\\n',\n",
              " '\\tÄr det vitt?\\n',\n",
              " '\\tÄr hon trevlig?\\n',\n",
              " '\\tÄr det där Tom?\\n',\n",
              " '\\tDet hände.\\n',\n",
              " '\\tDet kommer kanske att göra ont.\\n',\n",
              " '\\tDet kan komma att göra ont.\\n',\n",
              " '\\tDet är måndag.\\n',\n",
              " '\\tDet är molnigt.\\n',\n",
              " '\\tDet är meningslöst.\\n',\n",
              " '\\tDen är låst.\\n',\n",
              " '\\tDet är mitt jobb.\\n',\n",
              " '\\tDet är normalt.\\n',\n",
              " '\\tDet är vanligt.\\n',\n",
              " '\\tIn med dig bara.\\n',\n",
              " '\\tLåt mig hjälpa till.\\n',\n",
              " '\\tLåt mig leva.\\n',\n",
              " '\\tLivet är roligt.\\n',\n",
              " '\\tLunchen är klar.\\n',\n",
              " '\\tLunchen är färdig.\\n',\n",
              " '\\tIngen kom.\\n',\n",
              " '\\tSnälla kom.\\n',\n",
              " '\\tSnälla sluta!\\n',\n",
              " '\\tHon bet honom.\\n',\n",
              " '\\tHon slog honom.\\n',\n",
              " '\\tHon är snäll.\\n',\n",
              " '\\tStå still!\\n',\n",
              " '\\tStå stilla!\\n',\n",
              " '\\tStanna inomhus.\\n',\n",
              " '\\tRör dig inte.\\n',\n",
              " '\\tSluta försöka.\\n',\n",
              " '\\tTala långsammare.\\n',\n",
              " '\\tDet är okej.\\n',\n",
              " '\\tDet där är okej.\\n',\n",
              " '\\tDet är bra.\\n',\n",
              " '\\tSånt är livet.\\n',\n",
              " '\\tSådant är livet.\\n',\n",
              " '\\tDet är mitt.\\n',\n",
              " '\\tDet är okej.\\n',\n",
              " '\\tDet är sant.\\n',\n",
              " '\\tTv:n är på.\\n',\n",
              " '\\tDe ringde.\\n',\n",
              " '\\tDe kysste varandra.\\n',\n",
              " '\\tDet här är dåligt.\\n',\n",
              " '\\tTom fnissade.\\n',\n",
              " '\\tTom blev stor.\\n',\n",
              " '\\tTom blev biten.\\n',\n",
              " '\\tTom är tillbaka.\\n',\n",
              " '\\tTom är död.\\n',\n",
              " '\\tTom är döv.\\n',\n",
              " '\\tTom är fri.\\n',\n",
              " '\\tTom är fattig.\\n',\n",
              " '\\tTom är lång.\\n',\n",
              " '\\tTom är svag.\\n',\n",
              " '\\tTom klarade sig.\\n',\n",
              " '\\tTom lyckades.\\n',\n",
              " '\\tTom gjorde den.\\n',\n",
              " '\\tTom gjorde det.\\n',\n",
              " '\\tTom undervisar.\\n',\n",
              " '\\tTom var stor.\\n',\n",
              " '\\tTom ringer nog.\\n',\n",
              " '\\tTom kommer nog.\\n',\n",
              " '\\tTom hjälper nog till.\\n',\n",
              " '\\tTom överlever.\\n',\n",
              " '\\tTom överlever nog.\\n',\n",
              " '\\tTom tänker sluta.\\n',\n",
              " '\\tTom stannar nog.\\n',\n",
              " '\\tTom väntar nog.\\n',\n",
              " '\\tTom är alert.\\n',\n",
              " '\\tTom är på alerten.\\n',\n",
              " '\\tTom är uppmärksam.\\n',\n",
              " '\\tTom är snabb i vändningarna.\\n',\n",
              " '\\tTom är vid liv.\\n',\n",
              " '\\tTom är ensam.\\n',\n",
              " '\\tTom är allena.\\n',\n",
              " '\\tTom är för sig själv.\\n',\n",
              " '\\tTom är arg.\\n',\n",
              " '\\tTom är vaken.\\n',\n",
              " '\\tTom är uttråkad.\\n',\n",
              " '\\tTom är tokig.\\n',\n",
              " '\\tTom är galen.\\n',\n",
              " '\\tTom är full.\\n',\n",
              " '\\tTom dör.\\n',\n",
              " '\\tTom är tidig.\\n',\n",
              " '\\tTom är stollig.\\n',\n",
              " '\\tTom är hispig.\\n',\n",
              " '\\tTom är knäpp.\\n',\n",
              " '\\tTom är rolig.\\n',\n",
              " '\\tTom har tur.\\n',\n",
              " '\\tTom är lyckligt lottad.\\n',\n",
              " '\\tTom ljuger.\\n',\n",
              " '\\tTom är framfusig.\\n',\n",
              " '\\tTom är framåt.\\n',\n",
              " '\\tTom har rätt.\\n',\n",
              " '\\tTom är smart.\\n',\n",
              " '\\tTom är trött.\\n',\n",
              " '\\tTom har fel.\\n',\n",
              " '\\tTom är ung.\\n',\n",
              " '\\tStäng av den.\\n',\n",
              " '\\tHade jag fel?\\n',\n",
              " '\\tVi är här.\\n',\n",
              " '\\tVi åt ägg.\\n',\n",
              " '\\tVi gjorde slut.\\n',\n",
              " '\\tVi behöver dig.\\n',\n",
              " '\\tVi lovade.\\n',\n",
              " '\\tVi överlevde.\\n',\n",
              " '\\tVi är döende.\\n',\n",
              " '\\tVi ska åka.\\n',\n",
              " '\\tVi ska gå.\\n',\n",
              " '\\tVi åker.\\n',\n",
              " '\\tVi går.\\n',\n",
              " '\\tVi är färdiga.\\n',\n",
              " '\\tVi är klara.\\n',\n",
              " '\\tVi är redo.\\n',\n",
              " '\\tVad är nytt?\\n',\n",
              " '\\tVad kommer härnäst?\\n',\n",
              " '\\tVad ska ske nu?\\n',\n",
              " '\\tVad händer nu?\\n',\n",
              " '\\tVad är detta?\\n',\n",
              " '\\tVar är han?\\n',\n",
              " '\\tVem flydde?\\n',\n",
              " '\\tVem går först?\\n',\n",
              " '\\tVem är först i tur?\\n',\n",
              " '\\tVem står först i tur?\\n',\n",
              " '\\tDu är min.\\n',\n",
              " '\\tÄr jag förälskad?\\n',\n",
              " '\\tÄr du vilse?\\n',\n",
              " '\\tÄr du säker?\\n',\n",
              " '\\tÄr du utom fara?\\n',\n",
              " '\\tÄr du trygg?\\n',\n",
              " '\\tÄr du tillräknelig?\\n',\n",
              " '\\tÄr du vid dina sinnens fulla bruk?\\n',\n",
              " '\\tÄr ni tillräkneliga?\\n',\n",
              " '\\tÄr ni vid era sinnens fulla bruk?\\n',\n",
              " '\\tÄr du säker?\\n',\n",
              " '\\tÄr det säkert?\\n',\n",
              " '\\tKoka ett ägg.\\n',\n",
              " '\\tGå och hämta förstärkning.\\n',\n",
              " '\\tKan du komma?\\n',\n",
              " '\\tKan du simma?\\n',\n",
              " '\\tKom framåt.\\n',\n",
              " '\\tKom fram.\\n',\n",
              " '\\tKom fort!\\n',\n",
              " '\\tKom med oss.\\n',\n",
              " '\\tRingde Tom?\\n',\n",
              " '\\tHar Tom ringt?\\n',\n",
              " '\\tRöstade Tom?\\n',\n",
              " '\\tRöstade du?\\n',\n",
              " '\\tSer jag OK ut?\\n',\n",
              " '\\tSer jag bra ut?\\n',\n",
              " '\\tGör det på måndag.\\n',\n",
              " '\\tGör det ändå.\\n',\n",
              " '\\tHåller ni med?\\n',\n",
              " '\\tRöker ni?\\n',\n",
              " '\\tRöker du?\\n',\n",
              " '\\tSnarkar du?\\n',\n",
              " '\\tFråga inte mig.\\n',\n",
              " '\\tVar inte ledsen.\\n',\n",
              " '\\tSkämta inte med mig!\\n',\n",
              " '\\tSläpp inte taget.\\n',\n",
              " '\\tSkrik inte.\\n',\n",
              " '\\tKör fortare.\\n',\n",
              " '\\tKör försiktigt.\\n',\n",
              " '\\tVisitera dem.\\n',\n",
              " '\\tUndersök dem.\\n',\n",
              " '\\tUndersök den här.\\n',\n",
              " '\\tHitta katten.\\n',\n",
              " '\\tFisk, tack.\\n',\n",
              " '\\tGå tillbaka, Tom.\\n',\n",
              " '\\tGå dit nu.\\n',\n",
              " '\\tGod morgon.\\n',\n",
              " '\\tGod morgon!\\n',\n",
              " '\\tVäx upp, Tom.\\n',\n",
              " '\\tHåll ut, Tom.\\n',\n",
              " '\\tTa dig ett glas.\\n',\n",
              " '\\tTa en drink.\\n',\n",
              " '\\tTa en till.\\n',\n",
              " '\\tHan kan göra det.\\n',\n",
              " '\\tHan har en bil.\\n',\n",
              " '\\tHan har hund.\\n',\n",
              " '\\tHan har en hund.\\n',\n",
              " '\\tHan är poet.\\n',\n",
              " '\\tHan äter.\\n',\n",
              " '\\tHan älskar henne.\\n',\n",
              " '\\tHan drev med mig.\\n',\n",
              " '\\tHan var själv.\\n',\n",
              " '\\tHan var ensam.\\n',\n",
              " '\\tHan var modig.\\n',\n",
              " '\\tHan var mäktig.\\n',\n",
              " '\\tHan var jättebra.\\n',\n",
              " '\\tHan kommer att komma.\\n',\n",
              " '\\tHan kommer att gå.\\n',\n",
              " '\\tHan är så söt.\\n',\n",
              " '\\tHjälp mig, Tom.\\n',\n",
              " '\\tHjälp oss, Tom.\\n',\n",
              " '\\tVad spännande!\\n',\n",
              " '\\tVad patetiskt!\\n',\n",
              " '\\tSå romantiskt!\\n',\n",
              " '\\tHur går det med skolan?\\n',\n",
              " '\\tHur går det på skolan?\\n',\n",
              " '\\tHur går det i skolan?\\n',\n",
              " '\\tJag är hemma.\\n',\n",
              " '\\tJag är så sjuk.\\n',\n",
              " '\\tJag bad om ursäkt.\\n',\n",
              " '\\tJag åt kaviar.\\n',\n",
              " '\\tJag kan hämta den.\\n',\n",
              " '\\tJag kan försöka göra det.\\n',\n",
              " '\\tJag bryr mig inte.\\n',\n",
              " '\\tJag har inget emot det.\\n',\n",
              " '\\tJag hatar lögnare.\\n',\n",
              " '\\tJag har en katt.\\n',\n",
              " '\\tJag har en hund.\\n',\n",
              " '\\tJag har ett jobb.\\n',\n",
              " '\\tJag har ett barn.\\n',\n",
              " '\\tJag har en penna.\\n',\n",
              " '\\tJag måste gå.\\n',\n",
              " '\\tJag improviserade.\\n',\n",
              " '\\tJag känner till en väg.\\n',\n",
              " '\\tJag vet ett sätt.\\n',\n",
              " '\\tJag vet en väg.\\n',\n",
              " '\\tJag gillar frukt.\\n',\n",
              " '\\tJag tycker om frukt.\\n',\n",
              " '\\tJag älskar att resa.\\n',\n",
              " '\\tJag hittade på det.\\n',\n",
              " '\\tJag stannar kanske.\\n',\n",
              " '\\tJag behöver en hatt.\\n',\n",
              " '\\tJag behöver ett jobb.\\n',\n",
              " '\\tJag behöver en penna.\\n',\n",
              " '\\tJag behöver målarfärg.\\n',\n",
              " '\\tJag behövde dig.\\n',\n",
              " '\\tJag behövde er.\\n',\n",
              " '\\tJag ser en bok.\\n',\n",
              " '\\tJag fattar.\\n',\n",
              " '\\tJag förstår.\\n',\n",
              " '\\tJag fattar.\\n',\n",
              " '\\tJag förstår.\\n',\n",
              " '\\tJag promenerar mycket.\\n',\n",
              " '\\tJag går mycket.\\n',\n",
              " '\\tJag var hungrig.\\n',\n",
              " '\\tJag drog mig ur.\\n',\n",
              " '\\tJag kommer inte att förlora.\\n',\n",
              " '\\tJag flyttar inte.\\n',\n",
              " '\\tJag flyttar mig inte.\\n',\n",
              " '\\tJag flyttar inte på mig.\\n',\n",
              " '\\tJag undrar varför.\\n',\n",
              " '\\tJag klarar mig.\\n',\n",
              " '\\tJag kommer att vara här.\\n',\n",
              " '\\tJag är här.\\n',\n",
              " '\\tJag stannar här.\\n',\n",
              " '\\tJag fixar en.\\n',\n",
              " '\\tJag hämtar en.\\n',\n",
              " '\\tJag ska betala dig.\\n',\n",
              " '\\tJag ska spara det.\\n',\n",
              " '\\tJag är doktor.\\n',\n",
              " '\\tJag är konstnärlig.\\n',\n",
              " '\\tJag är konstnärligt lagd.\\n',\n",
              " '\\tJag är konstnärlig av mig.\\n',\n",
              " '\\tJag är upptagen nu.\\n',\n",
              " '\\tJag drunknar.\\n',\n",
              " '\\tJag flyr.\\n',\n",
              " '\\tJag är färdig.\\n',\n",
              " '\\tJag är hemsk.\\n',\n",
              " '\\tJag har slutat med öl.\\n',\n",
              " '\\tJag är så lycklig.\\n',\n",
              " '\\tJag är bäst.\\n',\n",
              " '\\tJag är kittlig.\\n',\n",
              " '\\tDet är mitt fel.\\n',\n",
              " '\\tJag är jättetjock.\\n',\n",
              " '\\tJag fick dig.\\n',\n",
              " '\\tJag har dig.\\n',\n",
              " '\\tGår måndag bra?\\n',\n",
              " '\\tGår det bra på måndag?\\n',\n",
              " '\\tFunkar måndag?\\n',\n",
              " '\\tFungerar måndag?\\n',\n",
              " '\\tÄr Tom galen?\\n',\n",
              " '\\tÄr det sant?\\n',\n",
              " '\\tÄr detta kärlek?\\n',\n",
              " '\\tDen var svart.\\n',\n",
              " '\\tDet var svart.\\n',\n",
              " '\\tDet var natt.\\n',\n",
              " '\\tDet var högljutt.\\n',\n",
              " '\\tDen var högljudd.\\n',\n",
              " '\\tDet var tyst.\\n',\n",
              " '\\tDen var tyst.\\n',\n",
              " '\\tDen var rund.\\n',\n",
              " '\\tDet var runt.\\n',\n",
              " '\\tDet är pinsamt.\\n',\n",
              " '\\tDet är läggdags.\\n',\n",
              " '\\tDen är äkta.\\n',\n",
              " '\\tDet är äkta.\\n',\n",
              " '\\tDen är autentisk.\\n',\n",
              " '\\tDet är autentiskt.\\n',\n",
              " '\\tDet haglar.\\n',\n",
              " '\\tDet är inget skämt.\\n',\n",
              " '\\tDet är konstigt.\\n',\n",
              " '\\tDen är konstig.\\n',\n",
              " '\\tDet är märkligt.\\n',\n",
              " '\\tDet är underligt.\\n',\n",
              " '\\tDen är för stor.\\n',\n",
              " '\\tDet är för varmt.\\n',\n",
              " '\\tDen fungerar.\\n',\n",
              " '\\tDet fungerar.\\n',\n",
              " '\\tDen funkar.\\n',\n",
              " '\\tDet funkar.\\n',\n",
              " '\\tFortsätt dansa.\\n',\n",
              " '\\tFortsätt springa.\\n',\n",
              " '\\tFortsätt le.\\n',\n",
              " '\\tFortsätt tala.\\n',\n",
              " '\\tFortsätt arbeta.\\n',\n",
              " '\\tDamerna först.\\n',\n",
              " '\\tLåt Tom leva.\\n',\n",
              " '\\tLåt Tom stanna.\\n',\n",
              " '\\tLåt mig göra det.\\n',\n",
              " '\\tMary kom in.\\n',\n",
              " '\\tMän är svin.\\n',\n",
              " '\\tDet gör ont i mina öron.\\n',\n",
              " '\\tBehöver jag fortsätta?\\n',\n",
              " '\\tIngen vet.\\n',\n",
              " '\\tIngen vet.\\n',\n",
              " '\\tSnälla ta det lugnt.\\n',\n",
              " '\\tSnälla lugna ner dig.\\n',\n",
              " '\\tPeka ut den.\\n',\n",
              " '\\tPeka ut det.\\n',\n",
              " '\\tHon kan simma.\\n',\n",
              " '\\tHon är lycklig.\\n',\n",
              " '\\tHon är tystlåten.\\n',\n",
              " '\\tHon är tyst.\\n',\n",
              " '\\tHon känner mig.\\n',\n",
              " '\\tHon gillade det.\\n',\n",
              " '\\tHon gillade den.\\n',\n",
              " '\\tHon tyckte om det.\\n',\n",
              " '\\tHon tyckte om den.\\n',\n",
              " '\\tHon gick ut.\\n',\n",
              " '\\tTala långsammare.\\n',\n",
              " '\\tStanna bilen.\\n',\n",
              " '\\tDet är mitt.\\n',\n",
              " '\\tDen är min.\\n',\n",
              " '\\tVad hemskt.\\n',\n",
              " '\\tDet är min cd-skiva.\\n',\n",
              " '\\tDet stämmer!\\n',\n",
              " '\\tDet är smart.\\n',\n",
              " '\\tVad konstigt.\\n',\n",
              " '\\tDe hurrade.\\n',\n",
              " '\\tDe störtade.\\n',\n",
              " '\\tDe kraschlandade.\\n',\n",
              " '\\tDe gjorde bankrutt.\\n',\n",
              " '\\tDe gick i kras.\\n',\n",
              " '\\tDe kraschade.\\n',\n",
              " '\\tDe vill ha mig.\\n',\n",
              " '\\tDe kommer att misslyckas.\\n',\n",
              " '\\tDe kommer att växa.\\n',\n",
              " '\\tDe är blå.\\n',\n",
              " '\\tDe är deppiga.\\n',\n",
              " '\\tDe är färdiga.\\n',\n",
              " '\\tDe är klara.\\n',\n",
              " '\\tDen här är min.\\n',\n",
              " '\\tTom blev klar.\\n',\n",
              " '\\tTom blev färdig.\\n',\n",
              " '\\tTom lyckades fly.\\n',\n",
              " '\\tTom kom hem.\\n',\n",
              " '\\tTom tog sig hem.\\n',\n",
              " '\\tTom gick vilse.\\n',\n",
              " '\\tTom har dött.\\n',\n",
              " '\\tTom har ryggrad.\\n',\n",
              " '\\tTom har kurage.\\n',\n",
              " '\\tTom hatade det.\\n',\n",
              " '\\tTom hatade den.\\n',\n",
              " '\\tTom hatar tv.\\n',\n",
              " '\\tTom hatar mig.\\n',\n",
              " '\\tTom hörde det.\\n',\n",
              " '\\tTom hjälper oss.\\n',\n",
              " '\\tTom anställde mig.\\n',\n",
              " '\\tTom slog Mary.\\n',\n",
              " '\\tTom lever.\\n',\n",
              " '\\tTom är vaken.\\n',\n",
              " '\\tTom är hemsk.\\n',\n",
              " '\\tTom är mörkhyad.\\n',\n",
              " '\\tTom är pank.\\n',\n",
              " '\\tTom är grym.\\n',\n",
              " '\\tTom är yr.\\n',\n",
              " '\\tTom ljuger.\\n',\n",
              " '\\tTom är naiv.\\n',\n",
              " '\\tTom behöver mig.\\n',\n",
              " '\\tTom sa ja.\\n',\n",
              " '\\tTom räddade mig.\\n',\n",
              " '\\tTom ryckte på axlarna.\\n',\n",
              " '\\tTom var mätt.\\n',\n",
              " '\\tTom var nyfiken.\\n',\n",
              " '\\tTom är road.\\n',\n",
              " '\\tTom gråter.\\n',\n",
              " '\\tTom äter.\\n',\n",
              " '\\tTom är känd.\\n',\n",
              " '\\tTom är smutsig.\\n',\n",
              " '\\tTom är lortig.\\n',\n",
              " '\\tTom är snål.\\n',\n",
              " '\\tTom är ostadig på benen.\\n',\n",
              " '\\tTom är groggy.\\n',\n",
              " '\\tTom är skyldig.\\n',\n",
              " '\\tTom är hungrig.\\n',\n",
              " '\\tTom är mentalsjuk.\\n',\n",
              " '\\tTom är sinnessjuk.\\n',\n",
              " '\\tTom är vansinnig.\\n',\n",
              " '\\tTom är vanvettig.\\n',\n",
              " '\\tTom skämtar.\\n',\n",
              " '\\tTom är ensam.\\n',\n",
              " '\\tTom är inte här.\\n',\n",
              " '\\tTom är inte på.\\n',\n",
              " '\\tTom är artig.\\n',\n",
              " '\\tTom är rädd.\\n',\n",
              " '\\tTom är tyst.\\n',\n",
              " '\\tTom är hög.\\n',\n",
              " '\\tTom är sträng.\\n',\n",
              " '\\tTom är stark.\\n',\n",
              " '\\tVi är araber.\\n',\n",
              " '\\tVi kommer överens.\\n',\n",
              " '\\tVi kommer väl överens.\\n',\n",
              " '\\tVi bor här.\\n',\n",
              " '\\tVi måste prata.\\n',\n",
              " '\\tVi måste pratas vid.\\n',\n",
              " '\\tVi måste tala.\\n',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbdxgjAOOnEf",
        "colab_type": "text"
      },
      "source": [
        "# **Engliag to German Transaltion**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdzwAhduO2R4",
        "colab_type": "code",
        "outputId": "fea6ee06-d914-428d-e691-39694aad774e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('deu.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "deu_sent = []\n",
        "eng_chars = set()\n",
        "deu_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and french sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    deu_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    deu_sent.append(deu_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in deu_line:\n",
        "        if (ch not in deu_chars):\n",
        "            deu_chars.add(ch)\n",
        "\n",
        "deu_chars = sorted(list(deu_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each french character - key is index and value is french character\n",
        "deu_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "deu_char_to_index_dict = {}\n",
        "for k, v in enumerate(deu_chars):\n",
        "    deu_index_to_char_dict[k] = v\n",
        "    deu_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_deu_sent = max([len(line) for line in deu_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_deu_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_deu_sentences = np.zeros(shape = (nb_samples,max_len_deu_sent,len(deu_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_deu_sent, len(deu_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(deu_sent[i]):\n",
        "        tokenized_deu_sentences[i,k,deu_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,deu_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(deu_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(deu_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_deu_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=10,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(deu_chars)))\n",
        "    target_seq[0, 0, deu_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent1 = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_deu_char = deu_index_to_char_dict[max_val_index]\n",
        "        translated_sent1 += sampled_deu_char\n",
        "        \n",
        "        if ( (sampled_deu_char == '\\n') or (len(translated_sent1) > max_len_deu_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(deu_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/10\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 1.1859 - val_loss: 1.1627\n",
            "Epoch 2/10\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.9229 - val_loss: 0.9316\n",
            "Epoch 3/10\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.7719 - val_loss: 0.8283\n",
            "Epoch 4/10\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.6978 - val_loss: 0.7758\n",
            "Epoch 5/10\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.6462 - val_loss: 0.7117\n",
            "Epoch 6/10\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.6078 - val_loss: 0.6741\n",
            "Epoch 7/10\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.5758 - val_loss: 0.6537\n",
            "Epoch 8/10\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.5494 - val_loss: 0.6282\n",
            "Epoch 9/10\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.5262 - val_loss: 0.6140\n",
            "Epoch 10/10\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.5054 - val_loss: 0.5991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB58Ud5gTWBS",
        "colab_type": "code",
        "outputId": "5f7ccfb5-915e-4739-bad0-f92c8d14e9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent1 = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: ﻿Go.\n",
            "Decoded sentence: Allez pour me chanter !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Courez-vous nouveau !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Courez-vous nouveau !\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Attaquez !\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Suisiez-moi l'aidendes.\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Suisis-loi d'aller mien !\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: Sais-tui !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Arrêtez ça !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Arrêtez ça !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Arrêtez ça !\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Attends !\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Attends !\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZTfXILsQFzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deu_sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRI3RbxAEWVb",
        "colab_type": "text"
      },
      "source": [
        "#**English to Finish Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjCTwqugEpQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('fin.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and french sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each french character - key is index and value is french character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rodang9rE_fV",
        "colab_type": "text"
      },
      "source": [
        "#**Englisg to Hungarian**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08iEirPNFJBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('hun.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and french sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each french character - key is index and value is french character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYS7uvIBFeDQ",
        "colab_type": "text"
      },
      "source": [
        "#**English to Italian Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNIApTThFwEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('ita.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and french sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each french character - key is index and value is french character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKJLvQjlFxia",
        "colab_type": "text"
      },
      "source": [
        "#**English to Dutch Taranslation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSq9C3N7HhzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('nld.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and french sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each french character - key is index and value is french character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJfwp27RHny2",
        "colab_type": "text"
      },
      "source": [
        "#**English to Portuguese Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ifs11AkIBX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('por.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and french sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each french character - key is index and value is french character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jEOYBfQINUH",
        "colab_type": "text"
      },
      "source": [
        "#**English to Russian Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXJL4ynKITnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('rus.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and french sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each french character - key is index and value is french character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6pYCwjKIaOj",
        "colab_type": "text"
      },
      "source": [
        "#**English to Spanish Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92dV8HPKIuJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('spa.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and french sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each french character - key is index and value is french character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrUoorj4JBTP",
        "colab_type": "text"
      },
      "source": [
        "#**English to Swedish Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Yd1pcnrJKB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('swe.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and french sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each french character - key is index and value is french character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EaVLtX7JSFE",
        "colab_type": "text"
      },
      "source": [
        "#**English to Turkish Translation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E7seQT6JSqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('tur.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and french sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each french character - key is index and value is french character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S7dd1XIJjzc",
        "colab_type": "text"
      },
      "source": [
        "#**English to Ukrainian Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDY86LnEKWrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "lines = open('ukr.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 10000\n",
        "\n",
        "# Process english and french sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line)\n",
        "    fra_sent.append(fra_line)\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)\n",
        "\n",
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))\n",
        "\n",
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each french character - key is index and value is french character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k \n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent]) \n",
        "\n",
        "max_len_eng_sent\n",
        "max_len_fra_sent\n",
        "\n",
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)\n",
        "\n",
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n",
        "\n",
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent\n",
        "\n",
        "for seq_index in range(12):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}